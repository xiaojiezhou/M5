{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "In my baseline validation set consist on the last 28 days of the training data. \n",
    "\n",
    "The problem in doing this is the following:\n",
    "\n",
    "Are we sure that this validation aligns with the test set?? (unknown demand and behaviour)\n",
    "\n",
    "We really don't know, thats what we need to predict xD.\n",
    "\n",
    "For this reason the best solution is to make a model that can generalize to unseen data (unknown demand and behaviour). Here we are going to validate the entire training data with GroupKFold strategy to avoid data leakage.\n",
    "\n",
    "The main point on doing this is that the mean of different validations can generalize to a wide range of cases, we will not have the best model for our test but we reduce the chance of overfitting and have a horrible score.\n",
    "\n",
    "Best model is to get a validation that is really similar to the test set, but we don't know which one it is, this is unknown, we can make some guesses or found a statistical ground to choose the right validation set.\n",
    "\n",
    "Another problem that we face is the loss function. Root mean squared error is not align with our competition metric (lower rmse does not guarantee a smaller wrmsse). For this we will use an asymmetric mean squared error loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/m5-reduce-data/data_small.pkl\n",
      "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/calendar.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from typing import Union\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading our data...\n",
      "Mem. usage decreased to 1540.89 Mb (57.1% reduction)\n",
      "We have 1011 days of training history\n",
      "we have 163 days left\n",
      "We have enought training data, lets continue\n",
      "Running simple feature engineering...\n",
      "Mem. usage decreased to 1027.26 Mb (58.5% reduction)\n",
      "Removing first 118 days\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model with 28 features...\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.06989\tvalid_1's custom_asymmetric_eval: 5.71725\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.64797\tvalid_1's custom_asymmetric_eval: 5.51806\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.43692\tvalid_1's custom_asymmetric_eval: 5.44275\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.28151\tvalid_1's custom_asymmetric_eval: 5.40021\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.16615\tvalid_1's custom_asymmetric_eval: 5.37357\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.06909\tvalid_1's custom_asymmetric_eval: 5.34705\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.98237\tvalid_1's custom_asymmetric_eval: 5.33064\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.90904\tvalid_1's custom_asymmetric_eval: 5.3194\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.83938\tvalid_1's custom_asymmetric_eval: 5.30696\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.7784\tvalid_1's custom_asymmetric_eval: 5.29318\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.722\tvalid_1's custom_asymmetric_eval: 5.28502\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.67086\tvalid_1's custom_asymmetric_eval: 5.27747\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.62374\tvalid_1's custom_asymmetric_eval: 5.27047\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.57725\tvalid_1's custom_asymmetric_eval: 5.26441\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.5342\tvalid_1's custom_asymmetric_eval: 5.25877\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.49366\tvalid_1's custom_asymmetric_eval: 5.25361\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.45711\tvalid_1's custom_asymmetric_eval: 5.25014\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.42086\tvalid_1's custom_asymmetric_eval: 5.24739\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.38697\tvalid_1's custom_asymmetric_eval: 5.24281\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.35456\tvalid_1's custom_asymmetric_eval: 5.23964\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.3233\tvalid_1's custom_asymmetric_eval: 5.23883\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.29292\tvalid_1's custom_asymmetric_eval: 5.23427\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.26407\tvalid_1's custom_asymmetric_eval: 5.23163\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.23587\tvalid_1's custom_asymmetric_eval: 5.23023\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.20922\tvalid_1's custom_asymmetric_eval: 5.22849\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.18422\tvalid_1's custom_asymmetric_eval: 5.22759\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's custom_asymmetric_eval: 3.19465\tvalid_1's custom_asymmetric_eval: 5.22701\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 4.99887\tvalid_1's custom_asymmetric_eval: 6.03064\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.61653\tvalid_1's custom_asymmetric_eval: 5.84026\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.40869\tvalid_1's custom_asymmetric_eval: 5.76111\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.25264\tvalid_1's custom_asymmetric_eval: 5.71539\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.13565\tvalid_1's custom_asymmetric_eval: 5.68247\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.0383\tvalid_1's custom_asymmetric_eval: 5.65515\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.95663\tvalid_1's custom_asymmetric_eval: 5.64157\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.88259\tvalid_1's custom_asymmetric_eval: 5.62274\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.81365\tvalid_1's custom_asymmetric_eval: 5.60608\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.75333\tvalid_1's custom_asymmetric_eval: 5.59387\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.6942\tvalid_1's custom_asymmetric_eval: 5.57829\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.64211\tvalid_1's custom_asymmetric_eval: 5.57126\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.5953\tvalid_1's custom_asymmetric_eval: 5.56141\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.5491\tvalid_1's custom_asymmetric_eval: 5.54976\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.50715\tvalid_1's custom_asymmetric_eval: 5.54352\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.46879\tvalid_1's custom_asymmetric_eval: 5.5392\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.43107\tvalid_1's custom_asymmetric_eval: 5.53215\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.39599\tvalid_1's custom_asymmetric_eval: 5.52775\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.36238\tvalid_1's custom_asymmetric_eval: 5.52071\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.33078\tvalid_1's custom_asymmetric_eval: 5.518\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.29894\tvalid_1's custom_asymmetric_eval: 5.51529\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.27073\tvalid_1's custom_asymmetric_eval: 5.51307\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.24403\tvalid_1's custom_asymmetric_eval: 5.50958\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.21674\tvalid_1's custom_asymmetric_eval: 5.50637\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.18991\tvalid_1's custom_asymmetric_eval: 5.5002\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.165\tvalid_1's custom_asymmetric_eval: 5.4967\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.14167\tvalid_1's custom_asymmetric_eval: 5.49486\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.11904\tvalid_1's custom_asymmetric_eval: 5.49303\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.09658\tvalid_1's custom_asymmetric_eval: 5.48985\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.07499\tvalid_1's custom_asymmetric_eval: 5.48835\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.05351\tvalid_1's custom_asymmetric_eval: 5.48536\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.03403\tvalid_1's custom_asymmetric_eval: 5.48685\n",
      "Early stopping, best iteration is:\n",
      "[1563]\ttraining's custom_asymmetric_eval: 3.04854\tvalid_1's custom_asymmetric_eval: 5.48505\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.1285\tvalid_1's custom_asymmetric_eval: 5.38466\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.71571\tvalid_1's custom_asymmetric_eval: 5.19654\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.49642\tvalid_1's custom_asymmetric_eval: 5.12873\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.35304\tvalid_1's custom_asymmetric_eval: 5.10594\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.22997\tvalid_1's custom_asymmetric_eval: 5.07588\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.12685\tvalid_1's custom_asymmetric_eval: 5.05404\n",
      "[350]\ttraining's custom_asymmetric_eval: 4.03854\tvalid_1's custom_asymmetric_eval: 5.03467\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.96535\tvalid_1's custom_asymmetric_eval: 5.02146\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.89457\tvalid_1's custom_asymmetric_eval: 5.01054\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.83049\tvalid_1's custom_asymmetric_eval: 4.99819\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.7699\tvalid_1's custom_asymmetric_eval: 4.98997\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.71763\tvalid_1's custom_asymmetric_eval: 4.98651\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.66699\tvalid_1's custom_asymmetric_eval: 4.97808\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.62006\tvalid_1's custom_asymmetric_eval: 4.97354\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.57456\tvalid_1's custom_asymmetric_eval: 4.9679\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.53321\tvalid_1's custom_asymmetric_eval: 4.9649\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.49395\tvalid_1's custom_asymmetric_eval: 4.96043\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.45909\tvalid_1's custom_asymmetric_eval: 4.95829\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.42389\tvalid_1's custom_asymmetric_eval: 4.95554\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.39178\tvalid_1's custom_asymmetric_eval: 4.95488\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.357\tvalid_1's custom_asymmetric_eval: 4.94917\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.32703\tvalid_1's custom_asymmetric_eval: 4.94904\n",
      "Early stopping, best iteration is:\n",
      "[1061]\ttraining's custom_asymmetric_eval: 3.35072\tvalid_1's custom_asymmetric_eval: 4.94846\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.05433\tvalid_1's custom_asymmetric_eval: 5.73137\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.65797\tvalid_1's custom_asymmetric_eval: 5.54351\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.43251\tvalid_1's custom_asymmetric_eval: 5.46497\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.27441\tvalid_1's custom_asymmetric_eval: 5.40607\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.14955\tvalid_1's custom_asymmetric_eval: 5.37632\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.05174\tvalid_1's custom_asymmetric_eval: 5.35536\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.96652\tvalid_1's custom_asymmetric_eval: 5.33931\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.89098\tvalid_1's custom_asymmetric_eval: 5.32612\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.82449\tvalid_1's custom_asymmetric_eval: 5.31399\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.76556\tvalid_1's custom_asymmetric_eval: 5.30259\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.71038\tvalid_1's custom_asymmetric_eval: 5.28544\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.65816\tvalid_1's custom_asymmetric_eval: 5.27663\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.6086\tvalid_1's custom_asymmetric_eval: 5.26671\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.56349\tvalid_1's custom_asymmetric_eval: 5.25959\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.51945\tvalid_1's custom_asymmetric_eval: 5.25045\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.47979\tvalid_1's custom_asymmetric_eval: 5.24689\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.44297\tvalid_1's custom_asymmetric_eval: 5.23944\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.40866\tvalid_1's custom_asymmetric_eval: 5.23337\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.37487\tvalid_1's custom_asymmetric_eval: 5.23094\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.34163\tvalid_1's custom_asymmetric_eval: 5.2263\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.31114\tvalid_1's custom_asymmetric_eval: 5.22451\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.28249\tvalid_1's custom_asymmetric_eval: 5.22294\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.25405\tvalid_1's custom_asymmetric_eval: 5.21978\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.22769\tvalid_1's custom_asymmetric_eval: 5.21674\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.20087\tvalid_1's custom_asymmetric_eval: 5.21501\n",
      "Early stopping, best iteration is:\n",
      "[1220]\ttraining's custom_asymmetric_eval: 3.21667\tvalid_1's custom_asymmetric_eval: 5.21491\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's custom_asymmetric_eval: 5.08052\tvalid_1's custom_asymmetric_eval: 5.52449\n",
      "[100]\ttraining's custom_asymmetric_eval: 4.67716\tvalid_1's custom_asymmetric_eval: 5.30669\n",
      "[150]\ttraining's custom_asymmetric_eval: 4.46376\tvalid_1's custom_asymmetric_eval: 5.23266\n",
      "[200]\ttraining's custom_asymmetric_eval: 4.3144\tvalid_1's custom_asymmetric_eval: 5.19094\n",
      "[250]\ttraining's custom_asymmetric_eval: 4.19312\tvalid_1's custom_asymmetric_eval: 5.15436\n",
      "[300]\ttraining's custom_asymmetric_eval: 4.08915\tvalid_1's custom_asymmetric_eval: 5.12558\n",
      "[350]\ttraining's custom_asymmetric_eval: 3.99981\tvalid_1's custom_asymmetric_eval: 5.09913\n",
      "[400]\ttraining's custom_asymmetric_eval: 3.91937\tvalid_1's custom_asymmetric_eval: 5.0795\n",
      "[450]\ttraining's custom_asymmetric_eval: 3.84869\tvalid_1's custom_asymmetric_eval: 5.05951\n",
      "[500]\ttraining's custom_asymmetric_eval: 3.78891\tvalid_1's custom_asymmetric_eval: 5.04693\n",
      "[550]\ttraining's custom_asymmetric_eval: 3.73035\tvalid_1's custom_asymmetric_eval: 5.03583\n",
      "[600]\ttraining's custom_asymmetric_eval: 3.67851\tvalid_1's custom_asymmetric_eval: 5.02632\n",
      "[650]\ttraining's custom_asymmetric_eval: 3.63136\tvalid_1's custom_asymmetric_eval: 5.01883\n",
      "[700]\ttraining's custom_asymmetric_eval: 3.58471\tvalid_1's custom_asymmetric_eval: 5.00945\n",
      "[750]\ttraining's custom_asymmetric_eval: 3.5423\tvalid_1's custom_asymmetric_eval: 5.00424\n",
      "[800]\ttraining's custom_asymmetric_eval: 3.50266\tvalid_1's custom_asymmetric_eval: 4.99618\n",
      "[850]\ttraining's custom_asymmetric_eval: 3.46465\tvalid_1's custom_asymmetric_eval: 4.99093\n",
      "[900]\ttraining's custom_asymmetric_eval: 3.42787\tvalid_1's custom_asymmetric_eval: 4.98266\n",
      "[950]\ttraining's custom_asymmetric_eval: 3.39307\tvalid_1's custom_asymmetric_eval: 4.97748\n",
      "[1000]\ttraining's custom_asymmetric_eval: 3.36093\tvalid_1's custom_asymmetric_eval: 4.97479\n",
      "[1050]\ttraining's custom_asymmetric_eval: 3.3287\tvalid_1's custom_asymmetric_eval: 4.97074\n",
      "[1100]\ttraining's custom_asymmetric_eval: 3.29755\tvalid_1's custom_asymmetric_eval: 4.96525\n",
      "[1150]\ttraining's custom_asymmetric_eval: 3.26953\tvalid_1's custom_asymmetric_eval: 4.96314\n",
      "[1200]\ttraining's custom_asymmetric_eval: 3.24213\tvalid_1's custom_asymmetric_eval: 4.9619\n",
      "[1250]\ttraining's custom_asymmetric_eval: 3.21566\tvalid_1's custom_asymmetric_eval: 4.95846\n",
      "[1300]\ttraining's custom_asymmetric_eval: 3.18921\tvalid_1's custom_asymmetric_eval: 4.9561\n",
      "[1350]\ttraining's custom_asymmetric_eval: 3.16452\tvalid_1's custom_asymmetric_eval: 4.95257\n",
      "[1400]\ttraining's custom_asymmetric_eval: 3.14041\tvalid_1's custom_asymmetric_eval: 4.94936\n",
      "[1450]\ttraining's custom_asymmetric_eval: 3.11794\tvalid_1's custom_asymmetric_eval: 4.94672\n",
      "[1500]\ttraining's custom_asymmetric_eval: 3.09544\tvalid_1's custom_asymmetric_eval: 4.94432\n",
      "[1550]\ttraining's custom_asymmetric_eval: 3.07362\tvalid_1's custom_asymmetric_eval: 4.94151\n",
      "[1600]\ttraining's custom_asymmetric_eval: 3.0536\tvalid_1's custom_asymmetric_eval: 4.93957\n",
      "[1650]\ttraining's custom_asymmetric_eval: 3.03376\tvalid_1's custom_asymmetric_eval: 4.93736\n",
      "[1700]\ttraining's custom_asymmetric_eval: 3.01458\tvalid_1's custom_asymmetric_eval: 4.93625\n",
      "[1750]\ttraining's custom_asymmetric_eval: 2.99522\tvalid_1's custom_asymmetric_eval: 4.93345\n",
      "[1800]\ttraining's custom_asymmetric_eval: 2.97666\tvalid_1's custom_asymmetric_eval: 4.93346\n",
      "[1850]\ttraining's custom_asymmetric_eval: 2.95911\tvalid_1's custom_asymmetric_eval: 4.93117\n",
      "[1900]\ttraining's custom_asymmetric_eval: 2.94108\tvalid_1's custom_asymmetric_eval: 4.9301\n",
      "[1950]\ttraining's custom_asymmetric_eval: 2.92364\tvalid_1's custom_asymmetric_eval: 4.93083\n",
      "Early stopping, best iteration is:\n",
      "[1903]\ttraining's custom_asymmetric_eval: 2.93984\tvalid_1's custom_asymmetric_eval: 4.92992\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Our out of folds rmse is 2.167256899855257\n",
      "Save predictions...\n"
     ]
    }
   ],
   "source": [
    "# helper functions to reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# function to read our data\n",
    "def read_data():\n",
    "    # read data\n",
    "    data = pd.read_pickle('/kaggle/input/m5-reduce-data/data_small.pkl')\n",
    "    # fillna and label encode categorical features\n",
    "    data = transform(data)\n",
    "    # read submission\n",
    "    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n",
    "    return data, submission\n",
    "\n",
    "# filla na and label encode categorical features\n",
    "def transform(data):\n",
    "    \n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "           'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "        \n",
    "    # reduce memory usage\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# simple feature ingineer function\n",
    "def simple_fe(data):\n",
    "    \n",
    "    data_fe = data[['id', 'demand']]\n",
    "    \n",
    "    window = 28\n",
    "    periods = [7, 15, 30, 90]\n",
    "    group = data_fe.groupby('id')['demand']\n",
    "    \n",
    "    # most recent lag data\n",
    "    for period in periods:\n",
    "        data_fe['demand_rolling_mean_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).mean())\n",
    "        data_fe['demand_rolling_std_t' + str(period)] = group.transform(lambda x: x.shift(window).rolling(period).std())\n",
    "        \n",
    "    # reduce memory\n",
    "    data_fe = reduce_mem_usage(data_fe)\n",
    "    \n",
    "    # get time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    time_features = ['year', 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear']\n",
    "    dtype = np.int16\n",
    "    for time_feature in time_features:\n",
    "        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "        \n",
    "    # concat lag and rolling features with main table\n",
    "    lag_rolling_features = [col for col in data_fe.columns if col not in ['id', 'demand']]\n",
    "    data = pd.concat([data, data_fe[lag_rolling_features]], axis = 1)\n",
    "    \n",
    "    del data_fe\n",
    "    gc.collect()\n",
    "\n",
    "    return data\n",
    "\n",
    "# define custom loss function\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n",
    "    hess = np.where(residual < 0, 2, 2 * 1.15)\n",
    "    return grad, hess\n",
    "\n",
    "# define custom evaluation metric\n",
    "def custom_asymmetric_valid(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False\n",
    "\n",
    "# define lgbm simple model\n",
    "def run_lgb(data, features, cat_features):\n",
    "    \n",
    "    # reset_index\n",
    "    data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-04-24']\n",
    "    y_train = x_train['demand']\n",
    "    test = data[data['date'] >= '2016-04-25']\n",
    "\n",
    "    # define random hyperparammeters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 1, \n",
    "        'colsample_bytree': 0.85,\n",
    "        'colsample_bynode': 0.85,\n",
    "        'min_data_per_leaf': 25,\n",
    "        'num_leaves': 200,\n",
    "        'lambda_l1': 0.5,\n",
    "        'lambda_l2': 0.5}\n",
    "    \n",
    "    oof = np.zeros(len(x_train))\n",
    "    preds = np.zeros(len(test))\n",
    "    \n",
    "    # GroupKFold by week, month to avoid leakage and overfitting (not entirely sure xD)\n",
    "    kf = GroupKFold(5)\n",
    "    # get subgroups for each week, year pair\n",
    "    group = x_train['week'].astype(str) + '_' + x_train['year'].astype(str)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(x_train, y_train, group)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_set = lgb.Dataset(x_train.iloc[trn_idx][features], y_train.iloc[trn_idx], \n",
    "                                categorical_feature = cat_features)\n",
    "        val_set = lgb.Dataset(x_train.iloc[val_idx][features], y_train.iloc[val_idx], \n",
    "                              categorical_feature = cat_features)\n",
    "        \n",
    "        # train with our custom loss function and evaluation metric\n",
    "        model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n",
    "                          valid_sets = [train_set, val_set], verbose_eval = 50, fobj = custom_asymmetric_train, \n",
    "                          feval = custom_asymmetric_valid)\n",
    "    \n",
    "        # predict oof\n",
    "        oof[val_idx] = model.predict(x_train.iloc[val_idx][features])\n",
    "\n",
    "        # predict test\n",
    "        preds += model.predict(test[features]) / 5\n",
    "        \n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "    oof_rmse = np.sqrt(metrics.mean_squared_error(y_train, oof))\n",
    "    print(f'Our out of folds rmse is {oof_rmse}')\n",
    "        \n",
    "    test = test[['id', 'date', 'demand']]\n",
    "    test['demand'] = preds\n",
    "    return test\n",
    "\n",
    "# function to get the predictions in the correct format\n",
    "def predict(test, submission):\n",
    "    predictions = pd.pivot(test, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission_custom_loss.csv', index = False)\n",
    "    \n",
    "# this is the main function that will run our entire program\n",
    "def train_and_evaluate():\n",
    "    \n",
    "    # read data\n",
    "    print('Reading our data...')\n",
    "    data, submission = read_data()\n",
    "    \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    # get amount of unique days in our data\n",
    "    days = abs((data['date'].min() - data['date'].max()).days)\n",
    "    # how many training data do we need to train with at least 2 years and consider lags\n",
    "    need = 365 + 365 + 90 + 28\n",
    "    print(f'We have {(days - 28)} days of training history')\n",
    "    print(f'we have {(days - 28 - need)} days left')\n",
    "    if (days - 28 - need) > 0:\n",
    "        print('We have enought training data, lets continue')\n",
    "    else:\n",
    "        print('Get more training data, training can fail')\n",
    "    \n",
    "    # simple feature engineer\n",
    "    print('Running simple feature engineering...')\n",
    "    data = simple_fe(data)\n",
    "    print('Removing first 118 days')\n",
    "    # eliminate the first 118 days of our train data because of lags\n",
    "    min_date = data['date'].min() + timedelta(days = 118)\n",
    "    data = data[data['date'] > min_date]\n",
    "    \n",
    "    # define our numeric features and categorical features\n",
    "    features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'year', \n",
    "                'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear', 'demand_rolling_mean_t7', 'demand_rolling_mean_t15', 'demand_rolling_mean_t30', 'demand_rolling_mean_t90',\n",
    "                'demand_rolling_std_t7', 'demand_rolling_std_t15', 'demand_rolling_std_t30', 'demand_rolling_std_t90']\n",
    "    \n",
    "    cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', \n",
    "                    'event_name_2', 'event_type_2']\n",
    "    \n",
    "    print('-'*50)\n",
    "    print('\\n')\n",
    "    print(f'Training model with {len(features)} features...')\n",
    "    # run lgbm model with 5 GroupKFold (subgroups by year, month)\n",
    "    test = run_lgb(data, features, cat_features)\n",
    "    print('Save predictions...')\n",
    "    # predict\n",
    "    predict(test, submission)\n",
    "        \n",
    "# run our program\n",
    "train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
